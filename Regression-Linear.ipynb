{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h(q) = q0c0 + q1x1 + q2x2 + .... qnxn\n",
    "The idea is to identify the best parameters/coefficients that minimize the cost function\n",
    "which is sum of squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways of solving linear regression\n",
    "Solve for the normal equation\n",
    "<br>\n",
    "https://www.geeksforgeeks.org/ml-normal-equation-in-linear-regression/\n",
    "<br>\n",
    "Or use gradient descent: (use this when the number of features is large or number of rows)\n",
    "Take partial derivate of the equation with respect to the other parameter to give optimal value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Metrics of evaluation\n",
    "r^2 = 1 - (Sum of squared errors/ Sum((actual_output-average_actual_output)^2)\n",
    "<br>\n",
    "Sum of squared errors : Gives how much the target variable varies around the line \n",
    "The denominator \n",
    "\n",
    "*Notes:\n",
    "<br>\n",
    "Removing outliers is key because a single point can skew this value and hence the r^2 metric can go off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding multicollinearity\n",
    "https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/\n",
    "It's basically corelation between multiple variables causing some regression in the model's ability to estimate the coefficients right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([1.0,2.3,4.5,6.7])\n",
    "y = np.array([1,2,3,4])\n",
    "pred = np.array([3,4,5,6])\n",
    "sum_of_errors = np.sum((pred-np.mean(y))**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model interpretability\n",
    "https://stattrek.com/regression/residual-analysis.aspx\n",
    "<br>\n",
    "http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/\n",
    "The residual analysis helps us understand if a non-linear solution should be tried or if there is no relationship\n",
    "between x and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heteroscedasticity \n",
    "https://statisticsbyjim.com/regression/heteroscedasticity-regression/\n",
    "Homoscedasticity is an assumption for linear regression\n",
    "When the residuals are plotted against the fitted value , the residuals across different range of fitted values should have constant variance, if it doesn't then there is probably heteroscedasticity which follows a cone like shape\n",
    "this is similar to model interpretability point but this indicates that the range of values is probably high and there is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept is at most times useless. It's a measure of what the average value of y should look like if x were 0. This probably doesn't hold any meaning when it comes to what kind of a prediction is made, but if you consider it solely as a regularization term, then it has purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways of solving linear regression:\n",
    "Way 1:\n",
    "\n",
    "https://are.berkeley.edu/courses/EEP118/current/derive_ols.pdf\n",
    "https://theclevermachine.wordpress.com/2012/09/01/derivation-of-ols-normal-equations/\n",
    "\n",
    "Way 2:\n",
    "https://mubaris.com/posts/linear-regression/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04306454]\n",
      " [0.0188617 ]\n",
      " [0.02793107]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.rand(100,1)\n",
    "z = np.random.rand(100,1)\n",
    "y = np.random.rand(100,1)\n",
    "features = np.array([0,0,0]).reshape((3,1))\n",
    "x0 = np.ones((100,1))\n",
    "\n",
    "\n",
    "# TODO : Rename features to weights\n",
    "X = np.array([x0,x,y]).T.reshape(len(x),3)\n",
    "m = len(X)\n",
    "def cost(X,y,features):\n",
    "    cost =  np.sum((np.dot(X,features) - y)**2) /(2*m)\n",
    "    return cost\n",
    "\n",
    "#print cost(X,y,features)\n",
    "learning_rate = 0.001\n",
    "\n",
    "def update_weights(X,y,features,iterations):\n",
    "    \"\"\"\n",
    "    Need cost per iteration and the gradient\n",
    "    gradient we learnt was 2*X*Error*learning rate\n",
    "    \"\"\"\n",
    "    iteration_cost = [0] * iterations\n",
    "    for index,iteration in enumerate(range(iterations)):\n",
    "        iteration_cost[index] = cost(X,y,features)\n",
    "        gradient = X.T.dot(np.dot(X, features)-y)/m\n",
    "        features = features - (learning_rate * gradient)\n",
    "    return features,iteration_cost\n",
    "        \n",
    "features, costs = update_weights(X,y,features,100)\n",
    "\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0]*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 15]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((1,2)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
