{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h(q) = q0c0 + q1x1 + q2x2 + .... qnxn\n",
    "The idea is to identify the best parameters/coefficients that minimize the cost function\n",
    "which is sum of squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways of solving linear regression\n",
    "Solve for the normal equation\n",
    "<br>\n",
    "https://www.geeksforgeeks.org/ml-normal-equation-in-linear-regression/\n",
    "<br>\n",
    "Or use gradient descent: (use this when the number of features is large or number of rows)\n",
    "Take partial derivate of the equation with respect to the other parameter to give optimal value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Metrics of evaluation\n",
    "r^2 = 1 - (Sum of squared errors/ Sum((actual_output-average_actual_output)^2)\n",
    "<br>\n",
    "Sum of squared errors : Gives how much the target variable varies around the line \n",
    "The denominator \n",
    "\n",
    "*Notes:\n",
    "<br>\n",
    "Removing outliers is key because a single point can skew this value and hence the r^2 metric can go off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding multicollinearity\n",
    "https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/\n",
    "It's basically corelation between multiple variables causing some regression in the model's ability to estimate the coefficients right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.0,2.3,4.5,6.7])\n",
    "y = np.array([1,2,3,4])\n",
    "pred = np.array([3,4,5,6])\n",
    "sum_of_errors = np.sum((pred-np.mean(y))**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model interpretability\n",
    "https://stattrek.com/regression/residual-analysis.aspx\n",
    "<br>\n",
    "http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/\n",
    "The residual analysis helps us understand if a non-linear solution should be tried or if there is no relationship\n",
    "between x and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heteroscedasticity \n",
    "https://statisticsbyjim.com/regression/heteroscedasticity-regression/\n",
    "Homoscedasticity is an assumption for linear regression\n",
    "When the residuals are plotted against the fitted value , the residuals across different range of fitted values should have constant variance, if it doesn't then there is probably heteroscedasticity which follows a cone like shape\n",
    "this is similar to model interpretability point but this indicates that the range of values is probably high and there is "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
